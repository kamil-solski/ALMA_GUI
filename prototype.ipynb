{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALMA inference\n",
    "\n",
    "The purpose of this project is to use ALMA model (write simple program to inference it through localhost web ui - no langchain, no complicated libraries, just simple webapp). \n",
    "\n",
    "This project could be also a way to test some solutions for web interfaces of more complicated DocWhisper project.\n",
    "\n",
    "1) we quantize model to smaller size (if X-ALMA will not be possible we could use GGUF version) - in other project we will learn how to quantize models to for example GGUF ourselves (patrz. BIBLIOTEKA_transformers-GPTs)\n",
    "    - how to quatize model?\n",
    "    - how to save it to for example GGUF?\n",
    "2) we distill knowledge to smaller model\n",
    "3) we create simple algo for inference\n",
    "4) we import our custom model \n",
    "\n",
    "It is not worth to lower precision to int8 for pre-quantize model for now (I might look into it in a future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to przetłumaczenia\n",
    "# Tak więc MoE łączy kilka innych modeli, aby wybrać, który ślad (część modelu) powinna zostać użyta do uzyskania prawidłowej odpowiedzi. Jest to technika optymalizacji. Agentic Workflow z drugiej strony to wiele niezależnych modeli, które działają w ramach jakiejś aplikacji, np. jeśli dane wejściowe to obraz, działają agenci wizji komputerowej itp. Bagging trenuje kilka różnych modeli, każdy na innym zbiorze danych i uśrednia wyniki, a boosting używa kilku różnych modeli, z których każdy poprawia błędy poprzedniego. Czy to wyjaśnienie jest poprawne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "from llama_cpp import Llama\n",
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = pathlib.Path.cwd() / \"Model\"\n",
    "\n",
    "GROUP2LANG = {\n",
    "1: [\"da\", \"nl\", \"de\", \"is\", \"no\", \"sv\", \"af\"],\n",
    "2: [\"ca\", \"ro\", \"gl\", \"it\", \"pt\", \"es\"],\n",
    "3: [\"bg\", \"mk\", \"sr\", \"uk\", \"ru\"],\n",
    "4: [\"id\", \"ms\", \"th\", \"vi\", \"mg\", \"fr\"],\n",
    "5: [\"hu\", \"el\", \"cs\", \"pl\", \"lt\", \"lv\"],\n",
    "6: [\"ka\", \"zh\", \"ja\", \"ko\", \"fi\", \"et\"],\n",
    "7: [\"gu\", \"hi\", \"mr\", \"ne\", \"ur\"],\n",
    "8: [\"az\", \"kk\", \"ky\", \"tr\", \"uz\", \"ar\", \"he\", \"fa\"],\n",
    "}\n",
    "\n",
    "LANG2GROUP = {lang: group for group, langs in GROUP2LANG.items() for lang in langs}\n",
    "group_id = LANG2GROUP[\"pl\"]\n",
    "\n",
    "model_name = f\"haoranxu/X-ALMA-13B-Group{group_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `haoranxu/X-ALMA-13B-Group5` from `transformers`...\n",
      "┌──────────────────────────────────────────────────────┐\n",
      "│Memory Usage for loading `haoranxu/X-ALMA-13B-Group5` │\n",
      "├───────┬─────────────┬──────────┬─────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│ Training using Adam │\n",
      "├───────┼─────────────┼──────────┼─────────────────────┤\n",
      "│float32│   1.18 GB   │ 47.88 GB │      191.51 GB      │\n",
      "│float16│  605.02 MB  │ 23.94 GB │       95.76 GB      │\n",
      "│  int8 │  302.51 MB  │ 11.97 GB │         N/A         │\n",
      "│  int4 │  151.25 MB  │ 5.98 GB  │         N/A         │\n",
      "└───────┴─────────────┴──────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!accelerate estimate-memory haoranxu/X-ALMA-13B-Group5 --library_name transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa82af4b81174be0834737e95a38c626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138a622ec1f146fdb26646a7079c2e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# II) way of loading model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, cache_dir=cache_dir).to(device)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Translate this from Polish to English:\\nPolish: Tak więc MoE łączy kilka innych modeli, aby wybrać, który ślad (część modelu) powinna zostać użyta do uzyskania prawidłowej odpowiedzi. Czy to wyjaśnienie jest poprawne? \\nEnglish\"\n",
    "\n",
    "chat_style_prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(chat_style_prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=600, truncation=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST] Translate this from Polish to English:\\nPolish: Tak więc MoE łączy kilka innych modeli, aby wybrać, który ślad (część modelu) powinna zostać użyta do uzyskania prawidłowej odpowiedzi. Czy to wyjaśnienie jest poprawne? \\nEnglish [/INST]Thus, the MoE combines several other models to choose which trace (part of the model) should be used to obtain the correct answer. Is this explanation correct?']\n"
     ]
    }
   ],
   "source": [
    "# Actual translation\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=200, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    return textract.process(file_path).decode(\"utf-8\")\n",
    "\n",
    "file_path = pathlib.Path.cwd() / \"Test_docs/Praca_magisterska.docx\"\n",
    "text = extract_text_from_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_style_prompt = [{\"role\": \"user\", \"content\": text}]\n",
    "prompt = tokenizer.apply_chat_template(chat_style_prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=600, truncation=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=200, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "output_file = \"output.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) wstawić automatyczne dostosowywanie max_new_tokens i max_lenght tak aby model zwracał output dla całego tekstu a nie tylko pierwszych 600 znakow\n",
    "#    - również funkcja powinna wczytywać dane tak aby oszczędzać miejsce w pamięci (coś w rodzaju batchy)\n",
    "# 2) przygotować prosty interfejs webowy w gradio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
