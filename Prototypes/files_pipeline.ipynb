{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from docx2python import docx2python\n",
    "import pdfplumber\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing docx extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalny skrypt wykorzystuje metodę A3C z biblioteki deepchem. W implementacji tej brakuje zwracanych metryk oraz progresu trenowania. W uczeniu przez wzmocnienie, skuteczna optymalizacja jest trudna bez szczegółowej analizy wydajności w miarę progresu uczenia. Aby to osiągnąć, monitoruje się szereg metryk, z których każda zapewnia wgląd w różne aspekty zachowania agenta. Metryki zaimplementowane w moich skryptach, obejmują kilka kluczowych wskaźników procesu uczenia się: skumulowane nagrody, średnie nagrody, zdyskontowane nagrody, entropię i długości epizodów. Wskaźniki te, często analizowane w wygładzonej (ang. Smoothed) formie, zapewniają kompleksowy obraz wydajności agenta, kompromisów między eksploracją a eksploatacją oraz wydajności treningu.\n",
      "\n",
      "Wygładzanie (ang. Smoothing) wykorzystuje średnią ruchomą, czyli każdy punkt danych jest zastępowany średnią sąsiednich punktów. Ilość punktów liczonych do tych średnich jest wyznaczana przez tzw. okno. Technika ta redukuje szumy i krótkoterminowe wahania, umożliwiając wyraźniejszą wizualizację trendów. Na przykład nagłe skoki nagród spowodowane przypadkowymi działaniami eksploracyjnymi mogą zaciemnić ogólną trajektorię progresu uczenia. Wygładzenie łagodzi te wartości odstające, zapewniając dokładniejsze odzwierciedlenie postępów w nauce agenta. Wybór odpowiedniego okna rozmiaru ma znaczenie z uwagi na tempo zmian. Zbyt małe okno zostawi nienaruszone szumy, zbyt duże okno niweluje widoczność nagłych zmian, wpływając przez to na interpretację.\n",
      "\n",
      "Skumulowane nagrody mierzą całkowitą nagrodę zgromadzoną przez agenta we wszystkich epizodach. Wskaźnik ten odzwierciedla długoterminowy sukces agenta w osiąganiu jego celów. Stały wzrost skumulowanych nagród wskazuje na skuteczne uczenie się i adaptację, podczas gdy płaskowyże lub spadki mogą oznaczać stagnację, lub wyzwania w uczeniu się (o ile agent nie osiągnął już maksymalnej wartości nagrody).\n",
      "\n",
      "\n",
      "\n",
      "----Image alt text----><----media/image1.png----\n",
      "\n",
      "Wyk.1 Wygładzone skumulowane nagrody dla agenta metody PPO. \n",
      "\n",
      "Na przedstawionym wykresie można wyróżnić dwie fazy. Faza początkowa (epizody 0-250), podczas której wartość skumulowanej nagrody zaczyna się od wartości najniższej równej 0,6 i stale rośnie, wskazując, że agent uczy się osiągać wyższe nagrody w miarę postępu szkolenia. Faza stabilizacji (epizody 250-2000), gdzie skumulowana nagroda osiąga wartość zbliżoną do maksymalnej (1,0). Sugeruje to, że agent osiągnął optymalną lub zbliżoną do optymalnej politykę. Obecność niewielkich wahań podczas tej fazy jest normalna i może wynikać z ciągłej eksploracji lub zmienności w środowisku. Widoczne są wahania na płaskowyżu (epizody 1000-2000), które są sporadyczne.  Spadki te mogą wskazywać na:\n",
      "\n",
      "--\t\t\tŚrodowisko wprowadzające elementy stochastyczne, sprawiające, że niektóre epizody są trudniejsze od innych.\n",
      "\n",
      "--\t\t\tAgent kontynuuje badanie nieco nieoptymalnych działań, co jest niezbędnym krokiem do udoskonalenia polityki.\n",
      "\n",
      "--\t\t\tMożliwe niestabilności w aktualizacjach polityki, choć powrót do normy sugeruje, że nie są one poważne.\n",
      "\n",
      "Wysoka wydajność (skumulowana nagroda w pobliżu 1,0), oznacza, że agent konsekwentnie osiąga dobre wyniki w osiąganiu celów, z niewielkimi odchyleniami. Oznacza to, że model ten nauczył się polityki, która bardzo dobrze generalizuje.\n",
      "\n",
      "\n",
      "\n",
      "Średnia nagroda na epizod reprezentuje natychmiastową wydajność agenta w każdym epizodzie. Wskaźnik ten zapewnia krótkoterminowy obraz tego, jak dobrze agent optymalizuje swoje działania. Stopniowy wzrost średnich nagród zazwyczaj oznacza, że agent uczy się podejmować lepsze decyzje w ramach ograniczeń środowiska.\n",
      "\n",
      "Discounted Rewards uwzględniają długoterminowy wpływ na akcje, poprzez stosowanie decay dla przyszłych nagród. Metryka ta ma kluczowe znaczenie dla agentów szkolonych ze współczynnikiem decay (γ), ponieważ odzwierciedla ich zdolność do równoważenia krótkoterminowych zysków z długotrwałymi korzyściami. \n",
      "\n",
      "Porównanie nagród średnich i discounted pomaga ocenić, czy agent równoważy nagrody krótko jak i długoterminowe oraz, czy wyuczona przez agenta jest skuteczna i stabilna.\n",
      "\n",
      "\n",
      "\n",
      "----Image alt text----><----media/image2.png----\n",
      "\n",
      "Wyk.2 Porównanie wygładzonych średnich nagród z discounted agenta PPO. \n",
      "\n",
      "Zarówno średnia nagroda, jak i zdyskontowane krzywe nagród wykazują podobne zachowanie oscylacyjne z ogólnym trendem wzrostowym we wczesnych epizodach, odzwierciedlając skuteczne uczenie się agenta PPO.\n",
      "\n",
      "Z biegiem czasu nagrody stabilizują się, ale nadal się wahają ze względu na nieodłączną stochastyczność uczenia się ze wzmocnieniem i kompromis między eksploracją a eksploatacją.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_result = docx2python('/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA.docx')\n",
    "\n",
    "text_with_placeholders = doc_result.text\n",
    "print(text_with_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first version - wrong json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first version\n",
    "def extract_pdf_with_placeholders(pdf_path, metadata_output_path):\n",
    "    sentences = []\n",
    "    metadata = {}\n",
    "    sentence_index = 0\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            words = page.extract_words()\n",
    "            images = page.images\n",
    "\n",
    "            # Collect elements with their positions\n",
    "            elements = [(float(w['top']), w['text']) for w in words]\n",
    "            elements += [(float(img['top']), '[IMAGE]') for img in images]\n",
    "            elements.sort(key=lambda x: x[0])\n",
    "\n",
    "            # Combine elements into one line and collect metadata\n",
    "            for position, content in elements:\n",
    "                sentences.append(content)\n",
    "                metadata[sentence_index] = {\n",
    "                    \"page\": page_num,\n",
    "                    \"position\": position,\n",
    "                    \"content\": content,\n",
    "                    \"is_image\": content == '[IMAGE]'\n",
    "                }\n",
    "                sentence_index += 1\n",
    "\n",
    "    # Save metadata to JSON\n",
    "    with open(metadata_output_path, 'w') as meta_file:\n",
    "        json.dump(metadata, meta_file, indent=4)\n",
    "\n",
    "    return \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalny skrypt wykorzystuje metodę A3C z biblioteki deepchem. W implementacji tej brakuje zwracanych metryk oraz progresu trenowania. W uczeniu przez wzmocnienie, skuteczna optymalizacja jest trudna bez szczegółowej analizy wydajności w miarę progresu uczenia. Aby to osiągnąć, monitoruje się szereg metryk, z których każda zapewnia wgląd w różne aspekty zachowania agenta. Metryki zaimplementowane w moich skryptach, obejmują kilka kluczowych wskaźników procesu uczenia się: skumulowane nagrody, średnie nagrody, zdyskontowane nagrody, entropię i długości epizodów. Wskaźniki te, często analizowane w wygładzonej (ang. Smoothed) formie, zapewniają kompleksowy obraz wydajności agenta, kompromisów między eksploracją a eksploatacją oraz wydajności treningu. Wygładzanie (ang. Smoothing) wykorzystuje średnią ruchomą, czyli każdy punkt danych jest zastępowany średnią sąsiednich punktów. Ilość punktów liczonych do tych średnich jest wyznaczana przez tzw. okno. Technika ta redukuje szumy i krótkoterminowe wahania, umożliwiając wyraźniejszą wizualizację trendów. Na przykład nagłe skoki nagród spowodowane przypadkowymi działaniami eksploracyjnymi mogą zaciemnić ogólną trajektorię progresu uczenia. Wygładzenie łagodzi te wartości odstające, zapewniając dokładniejsze odzwierciedlenie postępów w nauce agenta. Wybór odpowiedniego okna rozmiaru ma znaczenie z uwagi na tempo zmian. Zbyt małe okno zostawi nienaruszone szumy, zbyt duże okno niweluje widoczność nagłych zmian, wpływając przez to na interpretację. Skumulowane nagrody mierzą całkowitą nagrodę zgromadzoną przez agenta we wszystkich epizodach. Wskaźnik ten odzwierciedla długoterminowy sukces agenta w osiąganiu jego celów. Stały wzrost skumulowanych nagród wskazuje na skuteczne uczenie się i adaptację, podczas gdy płaskowyże lub spadki mogą oznaczać stagnację, lub wyzwania w uczeniu się (o ile agent nie osiągnął już maksymalnej wartości nagrody). [IMAGE] Wyk.1 Wygładzone skumulowane nagrody dla agenta metody PPO. Na przedstawionym wykresie można wyróżnić dwie fazy. Faza początkowa (epizody 0-250), podczas której wartość skumulowanej nagrody zaczyna się od wartości najniższej równej 0,6 i stale rośnie, wskazując, że agent uczy się osiągać wyższe nagrody w miarę postępu szkolenia. Faza stabilizacji (epizody 250-2000), gdzie skumulowana nagroda osiąga wartość zbliżoną do maksymalnej (1,0). Sugeruje to, że agent osiągnął optymalną lub zbliżoną do optymalnej politykę. Obecność niewielkich wahań podczas tej fazy jest normalna i może wynikać z ciągłej eksploracji lub zmienności w środowisku. Widoczne są wahania na płaskowyżu (epizody 1000-2000), które są sporadyczne. Spadki te mogą wskazywać na:  Środowisko wprowadzające elementy stochastyczne, sprawiające, że niektóre epizody są trudniejsze od innych.  Agent kontynuuje badanie nieco nieoptymalnych działań, co jest niezbędnym krokiem do udoskonalenia polityki.  Możliwe niestabilności w aktualizacjach polityki, choć powrót do normy sugeruje, że nie są one poważne. Wysoka wydajność (skumulowana nagroda w pobliżu 1,0), oznacza, że agent konsekwentnie osiąga dobre wyniki w osiąganiu celów, z niewielkimi odchyleniami. Oznacza to, że model ten nauczył się polityki, która bardzo dobrze generalizuje. Średnia nagroda na epizod reprezentuje natychmiastową wydajność agenta w każdym epizodzie. Wskaźnik ten zapewnia krótkoterminowy obraz tego, jak dobrze agent optymalizuje swoje działania. Stopniowy wzrost średnich nagród zazwyczaj oznacza, że agent uczy się podejmować lepsze decyzje w ramach ograniczeń środowiska. Discounted Rewards uwzględniają długoterminowy wpływ na akcje, poprzez stosowanie decay dla przyszłych nagród. Metryka ta ma kluczowe znaczenie dla agentów szkolonych ze współczynnikiem decay (γ), ponieważ odzwierciedla ich zdolność do równoważenia krótkoterminowych zysków z długotrwałymi korzyściami. Porównanie nagród średnich i discounted pomaga ocenić, czy agent równoważy nagrody krótko jak i długoterminowe oraz, czy wyuczona przez agenta jest skuteczna i stabilna. [IMAGE] Wyk.2 Porównanie wygładzonych średnich nagród z discounted agenta PPO. Zarówno średnia nagroda, jak i zdyskontowane krzywe nagród wykazują podobne zachowanie oscylacyjne z ogólnym trendem wzrostowym we wczesnych epizodach, odzwierciedlając skuteczne uczenie się agenta PPO. Z biegiem czasu nagrody stabilizują się, ale nadal się wahają ze względu na nieodłączną stochastyczność uczenia się ze wzmocnieniem i kompromis między eksploracją a eksploatacją.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA.pdf'\n",
    "metadata_output_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA_metadata.json'\n",
    "results = extract_pdf_with_placeholders(pdf_path, metadata_output_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second version - temporary chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_with_placeholders(pdf_path, metadata_path):\n",
    "    output_text = \"\"\n",
    "    metadata = []\n",
    "    sentence_index = 0\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            words = page.extract_words()\n",
    "            images = page.images\n",
    "\n",
    "            # Combine words and images by position\n",
    "            elements = [(float(w['top']), w['text']) for w in words]\n",
    "            elements += [(float(img['top']), '[/IMAGE]') for img in images]\n",
    "            \n",
    "            # Sort elements by vertical position\n",
    "            elements.sort(key=lambda x: x[0])\n",
    "            \n",
    "            page_text = \" \".join([content for _, content in elements])\n",
    "            \n",
    "            metadata.append({\n",
    "                \"page_number\": page_num,\n",
    "                \"start_sentence_index\": sentence_index\n",
    "            })\n",
    "            \n",
    "            output_text += page_text + \" \"\n",
    "            sentence_index += page_text.count('.') + page_text.count('!') + page_text.count('?')\n",
    "    \n",
    "    # Save metadata to JSON file\n",
    "    with open(metadata_path, 'w') as json_file:\n",
    "        json.dump(metadata, json_file, indent=4)\n",
    "    \n",
    "    return output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalny skrypt wykorzystuje metodę A3C z biblioteki deepchem. W implementacji tej brakuje zwracanych metryk oraz progresu trenowania. W uczeniu przez wzmocnienie, skuteczna optymalizacja jest trudna bez szczegółowej analizy wydajności w miarę progresu uczenia. Aby to osiągnąć, monitoruje się szereg metryk, z których każda zapewnia wgląd w różne aspekty zachowania agenta. Metryki zaimplementowane w moich skryptach, obejmują kilka kluczowych wskaźników procesu uczenia się: skumulowane nagrody, średnie nagrody, zdyskontowane nagrody, entropię i długości epizodów. Wskaźniki te, często analizowane w wygładzonej (ang. Smoothed) formie, zapewniają kompleksowy obraz wydajności agenta, kompromisów między eksploracją a eksploatacją oraz wydajności treningu. Wygładzanie (ang. Smoothing) wykorzystuje średnią ruchomą, czyli każdy punkt danych jest zastępowany średnią sąsiednich punktów. Ilość punktów liczonych do tych średnich jest wyznaczana przez tzw. okno. Technika ta redukuje szumy i krótkoterminowe wahania, umożliwiając wyraźniejszą wizualizację trendów. Na przykład nagłe skoki nagród spowodowane przypadkowymi działaniami eksploracyjnymi mogą zaciemnić ogólną trajektorię progresu uczenia. Wygładzenie łagodzi te wartości odstające, zapewniając dokładniejsze odzwierciedlenie postępów w nauce agenta. Wybór odpowiedniego okna rozmiaru ma znaczenie z uwagi na tempo zmian. Zbyt małe okno zostawi nienaruszone szumy, zbyt duże okno niweluje widoczność nagłych zmian, wpływając przez to na interpretację. Skumulowane nagrody mierzą całkowitą nagrodę zgromadzoną przez agenta we wszystkich epizodach. Wskaźnik ten odzwierciedla długoterminowy sukces agenta w osiąganiu jego celów. Stały wzrost skumulowanych nagród wskazuje na skuteczne uczenie się i adaptację, podczas gdy płaskowyże lub spadki mogą oznaczać stagnację, lub wyzwania w uczeniu się (o ile agent nie osiągnął już maksymalnej wartości nagrody). [IMAGE] Wyk.1 Wygładzone skumulowane nagrody dla agenta metody PPO. Na przedstawionym wykresie można wyróżnić dwie fazy. Faza początkowa (epizody 0-250), podczas której wartość skumulowanej nagrody zaczyna się od wartości najniższej równej 0,6 i stale rośnie, wskazując, że agent uczy się osiągać wyższe nagrody w miarę postępu szkolenia. Faza stabilizacji (epizody 250-2000), gdzie skumulowana nagroda osiąga wartość zbliżoną do maksymalnej (1,0). Sugeruje to, że agent osiągnął optymalną lub zbliżoną do optymalnej politykę. Obecność niewielkich wahań podczas tej fazy jest normalna i może wynikać z ciągłej eksploracji lub zmienności w środowisku. Widoczne są wahania na płaskowyżu (epizody 1000-2000), które są sporadyczne. Spadki te mogą wskazywać na:  Środowisko wprowadzające elementy stochastyczne, sprawiające, że niektóre epizody są trudniejsze od innych.  Agent kontynuuje badanie nieco nieoptymalnych działań, co jest niezbędnym krokiem do udoskonalenia polityki.  Możliwe niestabilności w aktualizacjach polityki, choć powrót do normy sugeruje, że nie są one poważne. Wysoka wydajność (skumulowana nagroda w pobliżu 1,0), oznacza, że agent konsekwentnie osiąga dobre wyniki w osiąganiu celów, z niewielkimi odchyleniami. Oznacza to, że model ten nauczył się polityki, która bardzo dobrze generalizuje. Średnia nagroda na epizod reprezentuje natychmiastową wydajność agenta w każdym epizodzie. Wskaźnik ten zapewnia krótkoterminowy obraz tego, jak dobrze agent optymalizuje swoje działania. Stopniowy wzrost średnich nagród zazwyczaj oznacza, że agent uczy się podejmować lepsze decyzje w ramach ograniczeń środowiska. Discounted Rewards uwzględniają długoterminowy wpływ na akcje, poprzez stosowanie decay dla przyszłych nagród. Metryka ta ma kluczowe znaczenie dla agentów szkolonych ze współczynnikiem decay (γ), ponieważ odzwierciedla ich zdolność do równoważenia krótkoterminowych zysków z długotrwałymi korzyściami. Porównanie nagród średnich i discounted pomaga ocenić, czy agent równoważy nagrody krótko jak i długoterminowe oraz, czy wyuczona przez agenta jest skuteczna i stabilna. [IMAGE] Wyk.2 Porównanie wygładzonych średnich nagród z discounted agenta PPO. Zarówno średnia nagroda, jak i zdyskontowane krzywe nagród wykazują podobne zachowanie oscylacyjne z ogólnym trendem wzrostowym we wczesnych epizodach, odzwierciedlając skuteczne uczenie się agenta PPO. Z biegiem czasu nagrody stabilizują się, ale nadal się wahają ze względu na nieodłączną stochastyczność uczenia się ze wzmocnieniem i kompromis między eksploracją a eksploatacją.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA.pdf'\n",
    "metadata_output_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA_metadata.json'\n",
    "results = extract_pdf_with_placeholders(pdf_path, metadata_output_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third version - included \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_with_placeholders(pdf_path, metadata_path):\n",
    "    output_text = \"\"\n",
    "    metadata = []\n",
    "    sentence_index = 0\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            words = page.extract_words(layout=True)\n",
    "            images = page.images\n",
    "\n",
    "            # Combine words and images by position\n",
    "            elements = [(float(w['top']), w['text'] + (\"\\n\" if w['doctop'] != words[i-1]['doctop'] else \"\")) for i, w in enumerate(words)]\n",
    "            elements += [(float(img['top']), '[IMAGE]\\n') for img in images]\n",
    "            \n",
    "            # Sort elements by vertical position\n",
    "            elements.sort(key=lambda x: x[0])\n",
    "            \n",
    "            page_text = \"\".join([content for _, content in elements])\n",
    "            \n",
    "            metadata.append({\n",
    "                \"page_number\": page_num,\n",
    "                \"start_sentence_index\": sentence_index\n",
    "            })\n",
    "            \n",
    "            output_text += page_text\n",
    "            sentence_index += page_text.count('.') + page_text.count('!') + page_text.count('?')\n",
    "    \n",
    "    # Save metadata to JSON file\n",
    "    with open(metadata_path, 'w') as json_file:\n",
    "        json.dump(metadata, json_file, indent=4)\n",
    "    \n",
    "    return output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WordExtractor.__init__() got an unexpected keyword argument 'layout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m metadata_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA_metadata.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mextract_pdf_with_placeholders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_output_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m, in \u001b[0;36mextract_pdf_with_placeholders\u001b[0;34m(pdf_path, metadata_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pdfplumber\u001b[38;5;241m.\u001b[39mopen(pdf_path) \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page_num, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf\u001b[38;5;241m.\u001b[39mpages, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m         words \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         images \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mimages\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Combine words and images by position\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ALMA/lib/python3.11/site-packages/pdfplumber/page.py:554\u001b[0m, in \u001b[0;36mPage.extract_words\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_words\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_obj_list:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_words\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ALMA/lib/python3.11/site-packages/pdfplumber/utils/text.py:698\u001b[0m, in \u001b[0;36mextract_words\u001b[0;34m(chars, return_chars, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_words\u001b[39m(\n\u001b[1;32m    696\u001b[0m     chars: T_obj_list, return_chars: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    697\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_obj_list:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWordExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mextract_words(chars, return_chars)\n",
      "\u001b[0;31mTypeError\u001b[0m: WordExtractor.__init__() got an unexpected keyword argument 'layout'"
     ]
    }
   ],
   "source": [
    "pdf_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA.pdf'\n",
    "metadata_output_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA_metadata.json'\n",
    "results = extract_pdf_with_placeholders(pdf_path, metadata_output_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth version - with layout\n",
    "TODO:\n",
    "- fix IMAGE position because it is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_with_placeholders(pdf_path):\n",
    "    output = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text(layout=True) or \"\"\n",
    "            images = page.images\n",
    "            \n",
    "            # Insert image placeholders inline\n",
    "            for img in images:\n",
    "                placeholder = '[IMAGE]'\n",
    "                text += f\"\\n{placeholder}\\n\"\n",
    "            \n",
    "            output.append(text)\n",
    "    \n",
    "    return \"\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "             Oryginalny skrypt wykorzystuje metodę A3C z biblioteki deepchem. W   \n",
      "                                                                                  \n",
      "        implementacji tej brakuje zwracanych metryk oraz progresu trenowania. W   \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        uczeniu przez wzmocnienie, skuteczna optymalizacja jest trudna bez        \n",
      "                                                                                  \n",
      "        szczegółowej analizy wydajności w miarę progresu uczenia. Aby to osiągnąć,\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        monitoruje się szereg metryk, z których każda zapewnia wgląd w różne aspekty\n",
      "                                                                                  \n",
      "        zachowania agenta. Metryki zaimplementowane w moich skryptach, obejmują   \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        kilka kluczowych wskaźników procesu uczenia się: skumulowane nagrody, średnie\n",
      "                                                                                  \n",
      "        nagrody, zdyskontowane nagrody, entropię i długości epizodów. Wskaźniki te,\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        często analizowane w wygładzonej (ang. Smoothed) formie, zapewniają       \n",
      "                                                                                  \n",
      "        kompleksowy obraz wydajności agenta, kompromisów między eksploracją a     \n",
      "                                                                                  \n",
      "        eksploatacją oraz wydajności treningu.                                    \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "             Wygładzanie (ang. Smoothing) wykorzystuje średnią ruchomą, czyli każdy\n",
      "                                                                                  \n",
      "        punkt danych jest zastępowany średnią sąsiednich punktów. Ilość punktów   \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        liczonych do tych średnich jest wyznaczana przez tzw. okno. Technika ta redukuje\n",
      "                                                                                  \n",
      "        szumy i krótkoterminowe wahania, umożliwiając wyraźniejszą wizualizację   \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        trendów. Na przykład nagłe skoki nagród spowodowane przypadkowymi         \n",
      "                                                                                  \n",
      "        działaniami eksploracyjnymi mogą zaciemnić ogólną trajektorię progresu uczenia.\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        Wygładzenie łagodzi te wartości odstające, zapewniając dokładniejsze      \n",
      "                                                                                  \n",
      "        odzwierciedlenie postępów w nauce agenta. Wybór odpowiedniego okna rozmiaru\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        ma znaczenie z uwagi na tempo zmian. Zbyt małe okno zostawi nienaruszone  \n",
      "                                                                                  \n",
      "        szumy, zbyt duże okno niweluje widoczność nagłych zmian, wpływając przez to\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        na interpretację.                                                         \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                                                                                                    \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "             Skumulowane nagrody mierzą całkowitą nagrodę zgromadzoną przez       \n",
      "                                                                                  \n",
      "        agenta we wszystkich epizodach. Wskaźnik ten odzwierciedla długoterminowy \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        sukces agenta w osiąganiu jego celów. Stały wzrost skumulowanych nagród   \n",
      "                                                                                  \n",
      "        wskazuje na skuteczne uczenie się i adaptację, podczas gdy płaskowyże lub \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        spadki mogą oznaczać stagnację, lub wyzwania w uczeniu się (o ile agent nie\n",
      "                                                                                  \n",
      "        osiągnął już maksymalnej wartości nagrody).                               \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        Wyk.1 Wygładzone skumulowane nagrody dla agenta metody PPO.               \n",
      "                                                                                  \n",
      "             Na przedstawionym wykresie można wyróżnić dwie fazy. Faza początkowa \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        (epizody 0-250), podczas której wartość skumulowanej nagrody zaczyna się od\n",
      "                                                                                  \n",
      "        wartości najniższej równej 0,6 i stale rośnie, wskazując, że agent uczy się osiągać\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        wyższe nagrody w miarę postępu szkolenia. Faza stabilizacji (epizody 250-2000),\n",
      "                                                                                  \n",
      "        gdzie skumulowana nagroda osiąga wartość zbliżoną do maksymalnej (1,0).   \n",
      "                                                                                  \n",
      "        Sugeruje to, że agent osiągnął optymalną lub zbliżoną do optymalnej politykę.\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "[IMAGE]\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        Obecność niewielkich wahań podczas tej fazy jest normalna i może wynikać z\n",
      "                                                                                  \n",
      "        ciągłej eksploracji lub zmienności w środowisku. Widoczne są wahania na   \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        płaskowyżu (epizody 1000-2000), które są sporadyczne. Spadki te mogą      \n",
      "                                                                                  \n",
      "        wskazywać na:                                                             \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "            Środowisko wprowadzające elementy stochastyczne, sprawiające, że     \n",
      "                                                                                  \n",
      "             niektóre epizody są trudniejsze od innych.                           \n",
      "                                                                                  \n",
      "            Agent kontynuuje badanie nieco nieoptymalnych działań, co jest       \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "             niezbędnym krokiem do udoskonalenia polityki.                        \n",
      "                                                                                  \n",
      "            Możliwe niestabilności w aktualizacjach polityki, choć powrót do normy\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "             sugeruje, że nie są one poważne.                                     \n",
      "                                                                                  \n",
      "        Wysoka wydajność (skumulowana nagroda w pobliżu 1,0), oznacza, że agent   \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        konsekwentnie osiąga dobre wyniki w osiąganiu celów, z niewielkimi        \n",
      "                                                                                  \n",
      "        odchyleniami. Oznacza to, że model ten nauczył się polityki, która bardzo dobrze\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        generalizuje.                                                             \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "             Średnia nagroda na epizod reprezentuje natychmiastową wydajność agenta\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        w każdym epizodzie. Wskaźnik ten zapewnia krótkoterminowy obraz tego, jak \n",
      "                                                                                  \n",
      "        dobrze agent optymalizuje swoje działania. Stopniowy wzrost średnich nagród\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        zazwyczaj oznacza, że agent uczy się podejmować lepsze decyzje w ramach   \n",
      "                                                                                  \n",
      "        ograniczeń środowiska.                                                    \n",
      "                                                                                  \n",
      "        Discounted Rewards uwzględniają długoterminowy wpływ na akcje, poprzez    \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        stosowanie decay dla przyszłych nagród. Metryka ta ma kluczowe znaczenie dla\n",
      "                                                                                  \n",
      "        agentów szkolonych ze współczynnikiem decay (γ), ponieważ odzwierciedla ich\n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                                                                                                    \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        zdolność do równoważenia krótkoterminowych zysków z długotrwałymi         \n",
      "                                                                                  \n",
      "        korzyściami.                                                              \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        Porównanie nagród średnich i discounted pomaga ocenić, czy agent równoważy\n",
      "                                                                                  \n",
      "        nagrody krótko jak i długoterminowe oraz, czy wyuczona przez agenta jest  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        skuteczna i stabilna.                                                     \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        Wyk.2 Porównanie wygładzonych średnich nagród z discounted agenta PPO.    \n",
      "                                                                                  \n",
      "             Zarówno średnia nagroda, jak i zdyskontowane krzywe nagród wykazują  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        podobne zachowanie oscylacyjne z ogólnym trendem wzrostowym we wczesnych  \n",
      "                                                                                  \n",
      "        epizodach, odzwierciedlając skuteczne uczenie się agenta PPO.             \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "        Z biegiem czasu nagrody stabilizują się, ale nadal się wahają ze względu na\n",
      "                                                                                  \n",
      "        nieodłączną stochastyczność uczenia się ze wzmocnieniem i kompromis między\n",
      "                                                                                  \n",
      "        eksploracją a eksploatacją.                                               \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "                                                                                  \n",
      "[IMAGE]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '/home/kamil-solski/Documents/Python/Projekty_py/ALMA/Test_docs/Test_ALMA.pdf'\n",
    "results = extract_pdf_with_placeholders(pdf_path)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - if we want to deal with images we could include layout and images at the end of creating new pdf with translation. We could use translation from chatbot, original pdf and metadata to inlude layout and images sentence wise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
